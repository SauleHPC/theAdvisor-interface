Instructions to push theAdvisor data to MongoDB.


## Upload citeseer raw data

check instructions in `mysql_stuff.md`

1. Located in CiteseerDumpTables run the `create_schema.sql` file.
        * This will create an authors, papers, paperVersions, and citations table.
2. Run the csx_db_7_15_2014_authors.sql, csx_db_7_15_2014_papers.sql, csx_db_7_15_2014_paperVersions.sql, and csx_db_7_15_2014_citations.sql.gz
  *these are generated from `extractTable.py`
3. From there execute mysqlmongo.py to move over each table from MySQL to a MongoDB collection.

some python packages needed:

```
pip3 install pandas sqlalchemy pymongo pymysql
```

in the csx_2014 datafile; authors is 12M tuples. papers is 4M tuples; paperVersions is 2.4M tuples; citations is 86M tuples;

pushing citation to mongo should take about 24 hours... :(

once you ahve the data in sql, you can extract the right file to pass for the matchign code `extract_citeseer_match_input_from_mysql.py`; that takes some time. maybe 30 minutes?

### Perform the matching within Citeseer to add an array of authors and an array of citations for each paper.
        * Use the citeseer_merge.py to add authors and citations array to each respective paper.
        * Change citeseer_collection variable to the collection you want to match (author or citations)
        * citations
                * When merging the citations make sure that fetch_specific_fields is set to true to only get the paperid's of the matches else set it to false.
                * For citations set both papers_collection_match_field and citeseer_collection_match_field set to 'cluster'.
        * authors
                * When merging the authors collection make sure to set set_specific_fields equal to false. 
                * For authors set papers_collection_match_field to 'id' and citeseer_collection_match_field set to 'paperid'.

## Push DBLP and MAG

For DBLP run `DBLP_to_mongo.py`
For MAG run `MAG_to_mongo.py`

        * Note make sure that parse.py and callback.py are in the same directory level as it uses functions from those scripts.

Note that the path to the DBLP file is currently hardcoded in `parse.py`
the MAG file is also currently hardcoded as `Papers.txt.gz`

seems like importing DBLP is about 30 minutes
seems like importing MAG is about an hour or so

## Push the matched files to MongoDB (DBLP->MAG and DBLP->Citeseer)
        * DBLP->MAG: Execute `upload_matched_files.py`, see usage
	  something like `python3 upload_matched_files.py DBLPtoMAG matchings/dblp_to_mag/*csv` 
        * DBLP->Citeseer: Execute `upload_matched_files.py`, see usage
	
