Instructions to push theAdvisor data to MongoDB.


## Upload citeseer raw data

check instructions in `mysql_stuff.md`

1. Located in CiteseerDumpTables run the `create_schema.sql` file.
        * This will create an authors, papers, paperVersions, and citations table.
2. Run the csx_db_7_15_2014_authors.sql, csx_db_7_15_2014_papers.sql, csx_db_7_15_2014_paperVersions.sql, and csx_db_7_15_2014_citations.sql.gz
  *these are generated from `extractTable.py`
3. From there execute mysqlmongo.py to move over each table from MySQL to a MongoDB collection.

some python packages needed:

```
pip3 install pandas sqlalchemy pymongo pymysql
```

in the csx_2014 datafile; authors is 12M tuples. papers is 4M tuples; paperVersions is 2.4M tuples; citations is 86M tuples;

pushing citation to mongo should take about 48 hours... :(

once you have the data in sql, you can extract the right file to pass for the matchign code `extract_citeseer_match_input_from_mysql.py`; that takes some time. maybe 30 minutes?

### Perform the matching within Citeseer to add an array of authors and an array of citations for each paper.
        * Use the citeseer_merge.py to add authors and citations array to each respective paper.
        * Change citeseer_collection variable to the collection you want to match (author or citations)
        * citations
                * When merging the citations make sure that fetch_specific_fields is set to true to only get the paperid's of the matches else set it to false.
                * For citations set both papers_collection_match_field and citeseer_collection_match_field set to 'cluster'.
        * authors
                * When merging the authors collection make sure to set set_specific_fields equal to false. 
                * For authors set papers_collection_match_field to 'id' and citeseer_collection_match_field set to 'paperid'.

## Push DBLP and MAG

For DBLP run `DBLP_to_mongo.py`
For MAG run `MAG_to_mongo.py`

        * Note make sure that parse.py and callback.py are in the same directory level as it uses functions from those scripts.

Note that the path to the DBLP file is currently hardcoded in `parse.py`
the MAG file is also currently hardcoded as `Papers.txt.gz`

seems like importing DBLP is about 30 minutes
seems like importing MAG is about an hour or so

You upload the mag references with `MAG_References_to_mongo.py`; that takes some time; there are 1419880598 mag edges


## Push the matched files to MongoDB (DBLP->MAG and DBLP->Citeseer)
        * DBLP->MAG: Execute `upload_matched_files.py`, see usage
	  something like `python3 upload_matched_files.py DBLPtoMAG matchings/dblp_to_mag/*csv` 
        * DBLP->Citeseer: Execute `upload_matched_files.py`, see usage
	


## create indexes

At some point we need to create indexes on all these mongo collections becasue the query time are stupid otherwise

also once the SQL stuff is done, one could flush the sql database

## build theadvisor dataset

Finally the script in `web/matching.py` puts everything together
It puts the theadvisor dataset together by build a graph of the matching between the different datasets.
It extracts connected components to identify each theadvisor paper individually.
Then it builds two collections:
-a reverse index `theadvisor_reverse` that serves to find from a particular MAG, Citeseer, or DBLP paper what its theadvisorid is
-a database of papers `theadvisor_papers` that stores all meta data on a theadvisor paper: title, authors, year, which MAG, Citeseer, or DBLP paper it is, and citation information split in citers and citee

This process is slow and memory hungry. It will run about 16 hours. And can take 15GB of memory.